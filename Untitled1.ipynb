{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: Shaon Bhatta Shuvo\n",
    "\"\"\"\n",
    "#Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as skd\n",
    "from sklearn import preprocessing\n",
    "import tensorflow.keras as tfk\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "\n",
    "#Generating synthetic linear dataset\n",
    "X,y = skd.make_regression(n_samples=100, n_features=1, n_targets=1, bias=0.5, noise=5.5, random_state=42)\n",
    "\n",
    "#Reading dataset from csv file\n",
    "#dataframe = pd.read_csv(\"datasetName.csv\", header=None) #header=0 if first row/line is the header of the dataset\n",
    "#dataset = dataframe.values\n",
    "#Split into input (X) and output (Y) variables,\n",
    "#Lets say the dataset have n number of columns where the last column is the target value\n",
    "#therefore we have first n-2 columns as independent variables (inputs) and the (n-1)th column is the as output since index starts from 0\n",
    "#X = dataset[:,0:n-1] # when range is [start:end] value is read from start to end-1 index\n",
    "#y = dataset[:,n-1] # here it is not range is specific index\n",
    "\n",
    "# Visulalizing the synthetic dataset\n",
    "print(\"\\nVisualizing the Synthetic Dataset\")\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.scatter(X,y,color='red',edgecolors=\"green\")\n",
    "plt.title(\"Synthetic Dataset\")\n",
    "plt.xlabel(\"X\", fontsize=20)\n",
    "plt.ylabel(\"y\",rotation = 0, fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "#reshaping the y values into 2D matrix of 1 column\n",
    "y = y.reshape(-1,1) #if y is not an array then use, np.asanyarray(y).reshape(-1,1)\n",
    "# Equivalent code y = np.reshape(y,(-1,1))\n",
    "\n",
    "#Feature Scaling (Standardization : needs 2D array as input)\n",
    "#Here your data Z is rescaled such that μ = 0 and 𝛔 = 1, and is done through this formula: z= (Xi - μ)/𝛔\n",
    "#sc = preprocessing.StandardScaler()\n",
    "#X = sc.fit_transform(X)\n",
    "#y = sc.fit_transform(y)\n",
    "\n",
    "# Visulalizing the synthetic dataset after standardization\n",
    "#print(\"\\nVisualizing the Synthetic Dataset after Standardization\")\n",
    "#plt.style.use(\"ggplot\")\n",
    "#plt.scatter(X,y,color='red',edgecolors=\"green\")\n",
    "#plt.title(\"Synthetic Dataset\")\n",
    "#plt.xlabel(\"X\", fontsize=20)\n",
    "#plt.ylabel(\"y\",rotation = 0, fontsize = 20)\n",
    "#plt.show()\n",
    "\n",
    "#Spliting the dataset into Training and Testset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#Creating the deep learning model with hyperbolic tangent activation function\n",
    "\n",
    "#A shape (5,2,8) means an array or tensor with 3 dimensions, containing 5 elements in the first dimension,\n",
    "#2 in the second and 8 in the third, totaling 30*4*10 = 1200 elements or numbers.\n",
    "#What flows between layers are tensors. Tensors can be seen as matrices, with shapes.\n",
    "#In Keras, the input layer itself is not a layer, but a tensor. It's the starting tensor you send to the first\n",
    "#hidden layer. This tensor must have the same shape as your training data.\n",
    "#In our example input data is one dimentional and also has only one element (column).\n",
    "model = tfk.Sequential([\n",
    "        tfk.Input(shape = (1,)),\n",
    "        tfk.layers.Dense(50, activation='tanh'),  #first hidden layer\n",
    "        tfk.layers.Dense(100, activation='tanh'), #second hidden layer\n",
    "        tfk.layers.Dense(1,) #activation linear by default, also can add: activation ='linear'\n",
    "        ])\n",
    "#model Looks like:  1 input -> [50 units in layer1] ->[100 units in layer2] -> 1 output\n",
    "\n",
    "#Compiling the model with Stochatstic Gradient Discent optimizer and MSE as the loss function\n",
    "model.compile(optimizer=tfk.optimizers.SGD(lr=0.001), loss='mean_squared_error', metrices=['mean_squared_error'])\n",
    "#Model's Summary\n",
    "model.summary()\n",
    "#Training the model\n",
    "training = model.fit(X_train,y_train, epochs = 50, batch_size =10)\n",
    "#Testing the models performance\n",
    "y_pred = model.predict(X_test)\n",
    "mse = metrics.mean_squared_error(y_test,y_pred)\n",
    "print(\"Testset Result: \\n---------------\")\n",
    "print(\"MSE: \", mse)\n",
    "\n",
    "#Visualizing training loss values\n",
    "plt.plot(training.history['loss'], label='Training Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "#Reshapping the matrix into array to pass the value into np.linspace() for 2D visualizaiton.\n",
    "x_train_arr = np.asarray(X_train).reshape(-1)\n",
    "y_train_arr = np.asarray(y_train).reshape(-1)\n",
    "x_test_arr = np.asarray(X_test).reshape(-1)\n",
    "y_test_arr = np.asarray(y_test).reshape(-1)\n",
    "#Creating evenly spaced values for smooth visulatization\n",
    "xp_train = np.linspace(x_train_arr.min(), x_train_arr.max())\n",
    "xp_test = np.linspace(x_test_arr.min(), x_test_arr.max())\n",
    "\n",
    "#Visulalizing training and testing plots.\n",
    "fig, ax = plt.subplots (nrows=1, ncols=2, figsize=(8, 4))\n",
    "ax[0].scatter (X_train, y_train, color='red', edgecolors='green', label='Synthetic Data Points')\n",
    "ax[0].plot(xp_train,model.predict(xp_train.reshape(-1)),color='blue', label='Regression Line')\n",
    "ax[0].set_title(\"NN Regression Plot (Training Set)\")\n",
    "ax[0].set_xlabel(\"X_train\", fontsize=20)\n",
    "ax[0].set_ylabel(\"y_train\", fontsize = 20)\n",
    "ax[0].legend()\n",
    "ax[1].scatter(X_test,y_test,color='red', edgecolors='green', label='Synthetic Data Points')\n",
    "ax[1].plot(xp_test,model.predict(xp_test.reshape(-1)),color='blue',label='Regression Line')\n",
    "ax[1].set_title(\"NN Regression Plot (Testing Set)\")\n",
    "ax[1].set_xlabel(\"X_test\", fontsize=20)\n",
    "ax[1].set_ylabel(\"y_test\", fontsize = 20)\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Creating Linear Regression Model\n",
    "lr_model = LinearRegression()\n",
    "#Training the model\n",
    "lr_model.fit(X_train,y_train)\n",
    "#Testing the model's performance\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "mse = metrics.mean_squared_error(y_test,y_pred_lr)\n",
    "print(\"MSE: \", mse)\n",
    "\n",
    "#Visualizing the Training and Testset performance of Linear Regression\n",
    "fig, ax = plt.subplots (nrows=1, ncols=2, figsize=(8, 4))\n",
    "ax[0].scatter (X_train, y_train, color='red', edgecolors='green', label='Synthetic Data Points')\n",
    "ax[0].plot(X_train,lr_model.predict(X_train),color='blue', label='Regression Line')\n",
    "ax[0].set_title(\"Linear Regression Plot (Training Set)\")\n",
    "ax[0].set_xlabel(\"X_train\", fontsize=20)\n",
    "ax[0].set_ylabel(\"y_train\", fontsize = 20)\n",
    "ax[0].legend()\n",
    "ax[1].scatter(X_test,y_test,color='red', edgecolors='green', label='Synthetic Data Points')\n",
    "ax[1].plot(X_test,lr_model.predict(X_test),color='blue',label='Regression Line')\n",
    "ax[1].set_title(\"Linear Regression Plot (Testing Set)\")\n",
    "ax[1].set_xlabel(\"X_test\", fontsize=20)\n",
    "ax[1].set_ylabel(\"y_test\", fontsize = 20)\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Generating synthetic non-linear data to solve classification problem\n",
    "X,y = skd.make_circles(n_samples=100, shuffle=False, noise=None, random_state=None, factor=0.5)\n",
    "\n",
    "#Following classes will not be shapped as circle, parameters can be changed to make it more non-linear\n",
    "#X, y = skd.make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2,\n",
    "#                             n_clusters_per_class=1,class_sep=0.5,flip_y=0.2, random_state=1,shuffle=False)\n",
    "\n",
    "#Finding and counting unique elements.\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique class of the dataset:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "\n",
    "#Visualizing the synthetic dataset of Class 1 and Class -1:\n",
    "plt.plot(X[:, 0][y == 0], X[:, 1][y == 0], 'g^', label='Class: 0')\n",
    "plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], 'ro' , label=\"Class: 1\")\n",
    "plt.title(\"Visualizing the synthetic dataset of class 1 and 0\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Spliting the dataset into train and testset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Frequency of unique class of elements in the test set:\")\n",
    "unique_elements_test, count_elements_test=np.unique(y_test, return_counts=True)\n",
    "print(unique_elements_test, count_elements_test)\n",
    "\n",
    "#Creating validation set  by copying last 10 elements from the training set\n",
    "X_val = X_train[70:]\n",
    "y_val = y_train[70:]\n",
    "#Removing the validation set (last 10 elements) from training set\n",
    "X_train = X_train[:70]\n",
    "y_train = y_train[:70]\n",
    "\n",
    "#Creating the deep learning model (Lets try a differnt apprach, can be used same approach shown earlier)\n",
    "model = tfk.Sequential()\n",
    "model.add(tfk.layers.Dense(50,input_shape=(2,), activation='relu')) #First Hidden Layer\n",
    "model.add(tfk.layers.Dense(100, activation='relu')) #Second  Hidden Layer\n",
    "model.add(tfk.layers.Dense(1, activation='sigmoid')) #Output Layer\n",
    "\n",
    "#Model can be crated using following approach as well\n",
    "#input_units = tfk.Input(shape=(2,))\n",
    "#hidden_layer1 = tfk.layers.Dense(100, activation ='relu')((input_units))\n",
    "#hidden_layer2 = tfk.layers.Dense(50, activation ='relu')(hidden_layer1)\n",
    "#prediction = tfk.layers.Dense(1, activation ='sigmoid')(hidden_layer2)\n",
    "#model = tfk.models.Model(inputs=input_units, outputs=prediction)\n",
    "\n",
    "#model Looks like:  2 input -> [50 units in layer1] ->[100 units in layer2] -> 1 output\n",
    "\n",
    "# Compiling the model for binary classification # Use loss = categorical_crossentropy for multiclass prediction.\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#Model's Summary\n",
    "model.summary()\n",
    "\n",
    "#Training the model\n",
    "training = model.fit(X_train,y_train, epochs = 50, batch_size =10, validation_data =(X_val,y_val))\n",
    "\n",
    "#Visulaizing the Training and Validation Sets Loss and Accuracy\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,4))\n",
    "#Plot training and validation accuracy values\n",
    "#axes[0].set_ylim(0,1) #if we want to limit axis in certain range\n",
    "axes[0].plot(training.history['accuracy'], label='Train')\n",
    "axes[0].plot(training.history['val_accuracy'], label='Validation')\n",
    "axes[0].set_title('Model Accuracy')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "#Plot training and validation loss values\n",
    "#axes[1].set_ylim(0,1)\n",
    "axes[1].plot(training.history['loss'], label='Train')\n",
    "axes[1].plot(training.history['val_loss'], label='Validation')\n",
    "axes[1].set_title('Model Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluating the performance on the Test set\n",
    "test_loss_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "# Visualising the Training and Test set plot decision area\n",
    "fig, axes = plt.subplots (nrows=1, ncols=2, figsize=(8, 4))\n",
    "fig1 = plot_decision_regions(X_train, y_train, clf=model, ax=axes[0], legend=0)\n",
    "fig2 = plot_decision_regions(X_test, y_test, clf=model, ax=axes[1], legend=0)\n",
    "axes[0].set_title('NN Plot Decision Region (Training set)')\n",
    "axes[0].set_xlabel('x1')\n",
    "axes[0].set_ylabel('x2')\n",
    "axes[1].set_title('NN Plot Decision Region (Test set)')\n",
    "axes[1].set_xlabel('x1')\n",
    "axes[1].set_ylabel('x2')\n",
    "\n",
    "handles, labels = fig1.get_legend_handles_labels()\n",
    "fig1.legend(handles,\n",
    "          ['class 0', 'class 1'])\n",
    "fig2.legend(handles,\n",
    "          ['class 0', 'class 1'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = model.predict(X_test)\n",
    "# Converting the predicted result into desired class level\n",
    "# Singmoid produce the output between 0 and 1. Therefore, the decision boundary for sigmoid is 0.5\n",
    "for i in range(0, len(y_pred)):\n",
    "    if(y_pred[i]>0.5):\n",
    "        y_pred[i] = 1\n",
    "    else:\n",
    "        y_pred[i] = 0\n",
    "\n",
    "# Generating confusion matrics, details classification report\n",
    "cm = metrics.confusion_matrix(y_test,y_pred)\n",
    "print(\"Confusion Matrix for Neural Network Model:\\n \",cm)\n",
    "print( \"{0}\".format(metrics.classification_report(y_test,y_pred)))\n",
    "# Generating accuracy in %,\n",
    "# Similary precision_score and recall_score can be used to generate precision and recall seperately\n",
    "accuracy_test = metrics.accuracy_score(y_test,y_pred)*100\n",
    "print('Accuracy:%.2f' % accuracy_test,\"%\")\n",
    "\n",
    "# Using non-linear svm calssifier , use kernel=linear for linear classifier.\n",
    "from sklearn import svm\n",
    "classifier = svm.SVC(kernel='rbf') #rbf = 'radial basis function' for non-linear classification\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "# Predicting Teset set result\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Generating confusion matrics, details classification report\n",
    "cm = metrics.confusion_matrix(y_test,y_pred)\n",
    "print(\"Confusion Matrix for SVM Clssifer:\\n \",cm)\n",
    "print( \"{0}\".format(metrics.classification_report(y_test,y_pred)))\n",
    "# Generating accuracy in %,\n",
    "# Similary precision_score and recall_score can be used to generate precision and recall seperately\n",
    "accuracy_test = metrics.accuracy_score(y_test,y_pred)*100\n",
    "print('Accuracy:%.2f' % accuracy_test,\"%\")\n",
    "\n",
    "# Visualising the Training and Test set plot decision area\n",
    "fig, axes = plt.subplots (nrows=1, ncols=2, figsize=(8, 4))\n",
    "fig1 = plot_decision_regions(X_train, y_train, clf=classifier, ax=axes[0], legend=0)\n",
    "fig2 = plot_decision_regions(X_test, y_test, clf=classifier, ax=axes[1], legend=0)\n",
    "axes[0].set_title('SVM Plot Decision Region (Training set)')\n",
    "axes[0].set_xlabel('x1')\n",
    "axes[0].set_ylabel('x2')\n",
    "axes[1].set_title('SVM Plot Decision Region (Test set)')\n",
    "axes[1].set_xlabel('x1')\n",
    "axes[1].set_ylabel('x2')\n",
    "handles, labels = fig1.get_legend_handles_labels()\n",
    "fig1.legend(handles,\n",
    "          ['class 0', 'class 1'])\n",
    "fig2.legend(handles,\n",
    "          ['class 0', 'class 1'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
